"""
Main Translator Pipeline Implementation

POC ê²€ì¦ëœ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©:
1. LangChain AgentExecutor + Tavily ê²€ìƒ‰ ë„êµ¬
2. ì´ë¯¸ì§€ ë¹„ì „ í¬ë§· ì§€ì›  
3. StateGraphë¡œ 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬
"""

import os
import yaml
import json
import logging
import asyncio
import base64
import cairosvg  # ğŸ”¥ SVG ë³€í™˜ì„ ìœ„í•´ ì¶”ê°€
from typing import Dict, Any, Optional, List, TypedDict
from pathlib import Path

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage


class GraphState(TypedDict):
    """ê·¸ë˜í”„ì˜ ì „ì²´ ìƒíƒœë¥¼ ì •ì˜ (POCì™€ ë™ì¼)"""
    # ì…ë ¥
    original_alt_text: str
    language: str
    image_type: str
    image_url: str
    
    # ìƒì„± ê³¼ì •
    on_the_fly_guidelines: Optional[List[str]]
    generated_alt_text: Optional[str]
    
    # í‰ê°€ ê³¼ì •
    feedback: Optional[str]
    accessibility_score: Optional[int]
    cultural_score: Optional[int]
    
    # ì—ëŸ¬ ì²˜ë¦¬
    error: Optional[str]


class OnTheFlyGuidelines(BaseModel):
    """Guideline Agentì˜ ì¶œë ¥ì„ ìœ„í•œ Pydantic ëª¨ë¸ (POCì™€ ë™ì¼)"""
    on_the_fly_guidelines: List[str] = Field(description="A list of 2-3 specific, on-the-fly cultural guidelines.")


class Evaluation(BaseModel):
    """Evaluatorì˜ ì¶œë ¥ì„ ìœ„í•œ Pydantic ëª¨ë¸ (POCì™€ ë™ì¼)"""
    accessibility_score: int = Field(description="The score for accessibility (1-5).")
    cultural_score: int = Field(description="The score for cultural appropriateness (1-5).")
    feedback: str = Field(description="If EITHER score is lower than 4, provide feedback. Otherwise, 'None'.")


class TranslatorPipeline:
    """
    POC ê²€ì¦ëœ 3ë‹¨ê³„ ë²ˆì—­ íŒŒì´í”„ë¼ì¸ì„ ë°±ì—”ë“œì— í†µí•©
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """
        ì´ˆê¸°í™” - POCì™€ ë™ì¼í•œ êµ¬ì¡°
        """
        if config_path is None:
            config_path = Path(__file__).parent / "translator.yaml"
        
        self.config_path = config_path
        self.config = self._load_config()
        
        # SVG ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.svg_cache_dir = "svg_data_cache"
        
        # LangChain ëª¨ë¸ ì´ˆê¸°í™” (POCì™€ ë™ì¼)
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3, streaming=True)
        self.agent_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
        
        # Tavily ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™” (POCì™€ ë™ì¼)
        self.tavily_tool = TavilySearchResults(max_results=3)
        self.tools = [self.tavily_tool]
        
        # ì²´ì¸ë“¤ ì´ˆê¸°í™”
        self._init_agents_and_chains()
        
        logging.info("TranslatorPipeline initialized with POC logic")
    
    def _load_config(self) -> Dict[str, Any]:
        """translator.yaml ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logging.info(f"Config loaded from {self.config_path}")
            return config
        except Exception as e:
            logging.error(f"Failed to load config from {self.config_path}: {e}")
            raise
    
    def _convert_svg_to_png_base64(self, svg_url: str) -> str:
        """
        SVG URLì„ PNGë¡œ ë³€í™˜í•˜ê³  base64 ë°ì´í„° URLë¡œ ë°˜í™˜
        """
        logging.info(f"Converting SVG to PNG: {svg_url}")
        
        # SVG ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        if not os.path.exists(self.svg_cache_dir):
            os.makedirs(self.svg_cache_dir)
        
        # íŒŒì¼ëª… ìƒì„±
        file_name = svg_url.split("/")[-1]
        png_filename = file_name.replace(".svg", ".png")
        png_path = os.path.join(self.svg_cache_dir, png_filename)
        
        # SVGë¥¼ PNGë¡œ ë³€í™˜
        cairosvg.svg2png(url=svg_url, write_to=png_path)
        
        # PNG íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©
        with open(png_path, 'rb') as png_file:
            png_data = base64.b64encode(png_file.read()).decode('utf-8')
        
        # ìºì‹œëœ PNG íŒŒì¼ ì‚­ì œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        try:
            os.remove(png_path)
        except Exception as e:
            logging.warning(f"Failed to remove cached PNG file: {e}")
        
        return f"data:image/png;base64,{png_data}"
    
    def _process_image_url(self, image_url: str) -> str:
        """
        ì´ë¯¸ì§€ URLì„ ì²˜ë¦¬í•˜ì—¬ OpenAI Vision APIê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
        """
        if image_url.strip().lower().endswith('.svg'):
            logging.info(f"SVG detected, converting to PNG: {image_url}")
            return self._convert_svg_to_png_base64(image_url)
        return image_url
    
    def _init_agents_and_chains(self):
        """POCì™€ ë™ì¼í•œ Agent ë° Chain ì´ˆê¸°í™”"""
        # Guideline Agent ìƒì„± (POCì™€ ë™ì¼)
        guideline_agent_prompt = ChatPromptTemplate.from_messages([
            ("system", self.config['guideline_synthesizer']['system']),
            ("user", [
                {"type": "text", "text": self.config['guideline_synthesizer']['user']},
                {"type": "image_url", "image_url": {"url": "{image_url}"}}
            ]),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        guideline_agent = create_openai_tools_agent(self.agent_llm, self.tools, guideline_agent_prompt)
        self.guideline_agent_executor = AgentExecutor(agent=guideline_agent, tools=self.tools, verbose=True)
        
        # Generator Chain ìƒì„± - culture_guidelines ì œê±°
        generator_system_prompt = self.config['generator']['system'].format(
            alt_text_guidelines=self.config['alt_text_guidelines'],
            on_the_fly_guidelines="{on_the_fly_guidelines}"
        )
        
        self.generation_prompt_template = ChatPromptTemplate.from_messages([
            ("system", generator_system_prompt),
            ("user", [
                {"type": "text", "text": self.config['generator']['user']},
                {"type": "image_url", "image_url": {"url": "{image_url}"}}
            ])
        ])
        self.generation_chain = self.generation_prompt_template | self.llm
        
        # Evaluator Chain ìƒì„± - culture_guidelines ì¶”ê°€
        evaluator_system_prompt = self.config['evaluator']['system'].format(
            alt_text_guidelines=self.config['alt_text_guidelines'],
            culture_guidelines=self.config['culture_guidelines']
        )
        
        evaluator_prompt = ChatPromptTemplate.from_messages([
            ("system", evaluator_system_prompt),
            ("user", [
                {"type": "text", "text": self.config['evaluator']['user']},
                {"type": "image_url", "image_url": {"url": "{image_url}"}}
            ])
        ])
        self.evaluation_chain = evaluator_prompt | self.agent_llm.with_structured_output(Evaluation)
    
    async def translate(
        self,
        original_alt_text: str,
        target_language_name: str,
        image_url: str,
        image_type: str = "informative"
    ) -> Dict[str, Any]:
        """
        ë©”ì¸ ë²ˆì—­ í•¨ìˆ˜ - POCì˜ StateGraph ë¡œì§ ì‚¬ìš©
        """
        logging.info(f"Starting translation pipeline: '{original_alt_text}' -> {target_language_name}")
        
        try:
            # POCì™€ ë™ì¼í•œ StateGraph êµ¬ì„±
            workflow = StateGraph(GraphState)
            
            # ë…¸ë“œ ì¶”ê°€ (POCì™€ ë™ì¼)
            workflow.add_node("guideline_agent", self._guideline_agent_node)
            workflow.add_node("generation_node", self._generation_node)
            workflow.add_node("evaluator", self._evaluator_node)
            
            # ì—£ì§€ ì—°ê²° (POCì™€ ë™ì¼)
            workflow.set_entry_point("guideline_agent")
            workflow.add_edge("guideline_agent", "generation_node")
            workflow.add_edge("generation_node", "evaluator")
            workflow.add_conditional_edges(
                "evaluator",
                self._should_continue,
                {"end": END, "generation_node": "generation_node"}
            )
            
            app = workflow.compile()
            
            # ì…ë ¥ ë°ì´í„° êµ¬ì„±
            inputs = {
                "original_alt_text": original_alt_text,
                "language": target_language_name,
                "image_type": image_type,
                "image_url": self._process_image_url(image_url)
            }
            
            # ê·¸ë˜í”„ ì‹¤í–‰ (POCì™€ ë™ì¼)
            final_state = {}
            for event in app.stream(inputs, {"recursion_limit": 50}):
                for key, value in event.items():
                    if key != "__end__":
                        final_state.update(value)
            
            return {
                "original_text": original_alt_text,
                "translated_text": final_state.get('generated_alt_text', original_alt_text),
                "target_language_name": target_language_name,
                "guidelines": final_state.get('on_the_fly_guidelines', []),
                "evaluation": {
                    "accessibility_score": final_state.get('accessibility_score'),
                    "cultural_score": final_state.get('cultural_score'),
                    "feedback": final_state.get('feedback')
                },
                "success": final_state.get('error') is None
            }
            
        except Exception as e:
            logging.error(f"Translation pipeline failed: {e}")
            return {
                "original_text": original_alt_text,
                "translated_text": original_alt_text,
                "target_language_name": target_language_name,
                "error": str(e),
                "success": False
            }
    
    def _guideline_agent_node(self, state: GraphState) -> GraphState:
        """Phase 1: ììœ¨ì ì¸ ì—ì´ì „íŠ¸ê°€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³ , ì›¹ ê²€ìƒ‰ì„ í†µí•´ ë§ì¶¤ ê°€ì´ë“œë¼ì¸ì„ ìƒì„± (POCì™€ ë™ì¼)"""
        logging.info("Step 1: Generating cultural guidelines with search")
        try:
            agent_vars = {
                "language": state["language"],
                "original_alt_text": state["original_alt_text"],
                "image_url": state["image_url"],  # ğŸ”¥ ì´ë¯¸ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ URL ì‚¬ìš©
            }
            result = self.guideline_agent_executor.invoke(agent_vars)
            
            # ì—ì´ì „íŠ¸ì˜ ì¶œë ¥ì—ì„œ JSON ë¶€ë¶„ë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ (POCì™€ ë™ì¼)
            json_str = result['output'][result['output'].find('{'):result['output'].rfind('}')+1]
            guideline_json = json.loads(json_str)
            
            # Pydanticìœ¼ë¡œ ìœ íš¨ì„± ê²€ì‚¬ ë° ë°ì´í„° ì¶”ì¶œ (POCì™€ ë™ì¼)
            on_the_fly_guidelines = OnTheFlyGuidelines(**guideline_json).on_the_fly_guidelines
            logging.info(f"Generated {len(on_the_fly_guidelines)} cultural guidelines")
            
            return {"on_the_fly_guidelines": on_the_fly_guidelines, "error": None}

        except Exception as e:
            logging.error(f"Guidelines generation failed: {e}")
            return {"error": str(e)}
    
    def _generation_node(self, state: GraphState) -> GraphState:
        """Phase 2: ìƒì„±ëœ ê°€ì´ë“œë¼ì¸ì— ë”°ë¼ ìµœì¢… Alt Textë¥¼ ìƒì„± (POCì™€ ë™ì¼)"""
        logging.info("Step 2: Generating translation with vision")
        try:
            g_vars = {
                "on_the_fly_guidelines": "\n".join([f"- {g}" for g in state["on_the_fly_guidelines"]]),
                "language": state["language"],
                "image_type": state["image_type"],
                "image_url": state["image_url"],  # ğŸ”¥ ì´ë¯¸ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ URL ì‚¬ìš©
                "original_alt_text": state["original_alt_text"],
                "previous_attempt": state.get("generated_alt_text", "N/A"),
                "feedback": state.get("feedback", "N/A")
            }

            # Vision ì…ë ¥ì„ í¬í•¨í•œ ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ ìƒì„± (POCì™€ ë™ì¼)
            prompt_with_vision = self.generation_prompt_template.invoke(g_vars)
            final_alt_text = self.llm.invoke(prompt_with_vision).content
            logging.info(f"Translation generated: '{final_alt_text}'")

            return {"generated_alt_text": final_alt_text, "error": None}

        except Exception as e:
            logging.error(f"Translation generation failed: {e}")
            return {"error": str(e)}
    
    def _evaluator_node(self, state: GraphState) -> GraphState:
        """ìƒì„±ëœ Alt Textë¥¼ Visionì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€ (POCì™€ ë™ì¼)"""
        logging.info("Step 3: Evaluating translation with vision")
        try:
            e_vars = {
                "image_url": state["image_url"],  # ğŸ”¥ ì´ë¯¸ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ URL ì‚¬ìš©
                "generated_alt_text": state["generated_alt_text"],
                "image_type": state["image_type"],
                "language": state["language"]
            }
            evaluation = self.evaluation_chain.invoke(e_vars)

            logging.info(f"Evaluation - Accessibility: {evaluation.accessibility_score}, Cultural: {evaluation.cultural_score}")

            return {
                "accessibility_score": evaluation.accessibility_score,
                "cultural_score": evaluation.cultural_score,
                "feedback": evaluation.feedback,
                "error": None
            }
        except Exception as e:
            logging.error(f"Translation evaluation failed: {e}")
            return {"error": str(e)}
    
    def _should_continue(self, state: GraphState) -> str:
        """í‰ê°€ ì ìˆ˜ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì • (POCì™€ ë™ì¼)"""
        logging.info("Checking quality threshold")
        if state.get("error"):
            logging.error(f"Error detected: {state['error']}, ending graph")
            return "end"
        
        if state.get("accessibility_score") is None or state.get("cultural_score") is None:
            logging.warning("Scores not found, ending graph to prevent loop")
            return "end"

        if state["accessibility_score"] < 4 or state["cultural_score"] < 4:
            logging.info(f"Threshold failed (A:{state['accessibility_score']}, C:{state['cultural_score']}). Looping back")
            return "generation_node"  # í”¼ë“œë°±ì„ ê°€ì§€ê³  generation_nodeë¡œ ëŒì•„ê°
        else:
            logging.info(f"Threshold passed (A:{state['accessibility_score']}, C:{state['cultural_score']})")
            return "end"


# í¸ì˜ í•¨ìˆ˜ë“¤
async def translate_with_pipeline(
    original_alt_text: str,
    target_language_name: str,
    image_url: str,
    image_type: str = "informative"
) -> Dict[str, Any]:
    """
    ì „ì²´ ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - ì „ì²´ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    """
    translator = TranslatorPipeline()
    result = await translator.translate(
        original_alt_text=original_alt_text,
        target_language_name=target_language_name,
        image_url=image_url,
        image_type=image_type
    )
    return result 